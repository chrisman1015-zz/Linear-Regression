---
title: "Linear Regression Modeling for Vintage Bordeaux"
author: "Christopher Hauman"
date: "March 16, 2018"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

For this project, I'm going to run through an example of fitting a multiple linear regression model in R. We're going to see if we can use this to accurately predict the price of Vintage Bordeaux wine each year based on a few measured variables for each vintage. To do this, we'll use variety of statistical and visual methods to evaluate the model, as well as perform a transformation to see if that improves the fit. If you're unfamiliar with any of the tests or methods I use, a quick Google search will yield numerous great resources for each concept.


First, let's take care of some housekeeping by setting the working directory and importing the csv file:

```{r}
## Set Working Directory
  setwd("/Users/Chris/Documents/R/Linear Regression Modeling for Vintage Bordeaux")
	getwd()

## Import Data
	data <- read.csv(file.path("Bordeaux.csv"), header = TRUE,sep=",")
	data
```
```{r}
```
## Part 1: Construct a Regression Model that Describes the Price of the Vintage

Let's create a multiple linear model of the price of Vintage Bordeaux as a function of its Age (in years where 1983=0), Rainfall (mm during the harvest season), Previous Rainfall (mm in the six months before the harvest season), and Temperature (degrees Celius during the growing season).
```{r}
y <- data$Price
x1 <- data$Age
x2 <- data$Rain
x3 <- data$PrevRain
x4 <- data$Temp

# multiple linear regression
fit <- lm(Price ~ Age + Rain + PrevRain + Temp, data = data)

```

```{r}
```
## Part 2: Run a Regression Analysis on the Data

Now we need to run a summary of the multiple linear regression to find the quantiles, coefficients, R-squared value, adjusted R-sqared value, F-Statistic, and the p-values. We'll then run an ANOVA table to get the variance data.

```{r}
summary(fit)
summary(aov(fit))
```

Note the R-Squared value is only 0.7356, which means only 73.56% of the variance in price is predicted from the current model. This is not particularly high, and should make us question the efficacy of the current linear model.


Let's also check the four model assumptions to see if the model is appropriate. These four assumptions are linearity, independence, normality of residuals, and equal variance of residuals.

```{r}
```
## Part 3: Check the Model Assumptions

```{r}
## 1. check for linearity
layout(matrix(c(1,2,3,4),2,2))

## run simple plots with each variable to check for linearity
plot(y ~ x1, main = "Price vs Age", xlab = "Age (years)", ylab = "Price")
plot(y ~ x2, main = "Price vs Rainfall", xlab = "Rainfall (mm)", ylab = "Price")
plot(y ~ x3, main = "Price vs Previous Rain", xlab = "Previous Rainfall (mm)", ylab = "Price")
plot(y ~ x4, main = "Price vs Temperature", xlab = "Temperature (degrees C)", ylab = "Price")

## Residual Plots e_i vs x_i
par(mfrow = c(2,2))
plot(resid(fit)~x1, main = "Residuals vs Age", 
     xlab = "Age (years)", ylab = "Residuals")
  abline(h = 0)
plot(resid(fit)~x2, main = "Residuals vs Rainfall",
     xlab = "Rainfall (mm)", ylab = "Residuals")
	abline(h = 0)
plot(resid(fit)~x3, main = "Residuals vs Previous Rain",
     xlab = "Previous Rainfall (mm)", ylab = "Residuals")
	abline(h = 0)
plot(resid(fit)~x4, main = "Residuals vs Temperature",
     xlab = "Temperature (degrees C)", ylab = "Residuals")
	abline(h = 0)

```

We see no evidence of linearity. There is not obvious relationship between any of the possible explanatory variables and the response variable (price) in any of the scatterplots. Furthermore, the patterns in the residual plots indicate that a linear relationship is not likely. We want to see a random distribution in the residual plots, which none of them have (the closest is price vs age, but it's still clearly not random).


Next, we can run a Lag 1 Autocorrelation to see if the independence of errors is reasonable:
```{r}
## 2. Check for Independence
	
##Lag 1 Autocorrelation
n = length(y)
acf(resid(fit), lag.max = 1, plot = FALSE)
2 / sqrt(n)
```

Since 0.588 > 0.3849 independence of variables is not necessarily reasonable. This violates the second condition for linearity.

Next, we'll checked for the normality of residuals with a q-q plot:

```{r}
## 3. Check for normality of Residuals

# q-q plot
z <- ( fit$residuals - mean(fit$residuals) ) / sd(fit$residuals)
qqnorm(z, sub = "Residuals")
abline(a = 0, b = 1)

```

The q-q plot does not appear to be linear. We can say that errors are not necessarily normally distributed (it's very likely that they aren't).

Finally, we'll check the equal variance of residuals with a plot of the residuals against fitted values:
```{r}

# residual plots
layout(matrix(c(1,2,3,4),2,2))
plot(fit)

```

We see a pattern in the plot for the residuals vs fitted value, so it's not likely that the errors have equal variances.

So, we have no evidence that our current model adheres to any of the four basic model assumptions. It appears the current model is not adequate.

```{r}
```

## Part 4: Transformation 

To try and resolve this failure, we'll apply a transformation to y. To perform the transformation, we can first run a box-cox power transformation to find the most appropriate transformation for y.
```{r}
## Load additional library
	library(MASS)

# Plot of Log-likelihood function vs lambda
	boxcox(y ~ x1 + x2 +x3 + x4,
            plotit=T, lambda = seq(-2, 2, length = 10))
	## Looking for lambda values 95% and above
	## The "best" estimate for lambda is where the curve
	## is at maximum (lambda approximately 0)
	## A 95% confidence interval for lambda is
	## approximately -1.3 to 1.3


## List of values of lambda and corresponding Log-likelihood function
	boxcox(y ~ x1 + x2 +x3 + x4,
            plotit=F, lambda = seq(0, 0.5, length = 10))

	lambda <- 0


## Box-Cox power transformation
	trans.y <- log(y)
	trans.y	## transformed y values
	y	## original dataset y values

## Linear regression of the transformated data
	trans.fit <- lm(trans.y ~ Age + Rain + PrevRain + Temp, data = data)

```
We pick the value of lamda with the largest log-likelihood (y-value). -7.938316 is largest and corresponds with approximately x = 0. So lamda = 0. This implies the best transformation for y is log(y).

Now we have an updated model. We'll then apply the previous tests again to see if the results were better.

```{r}
```
## Part 5: Re-run Regression Analysis

First, let's look again at the simple plots for the new model to compare:
```{r}
## 1. check for linearity
layout(matrix(c(1,2,3,4),2,2))

## run simple plots with each variable to check for linearity
plot(trans.y ~ x1, main = "log(Price) vs Age", xlab = "Age (years)", ylab = "log(Price)")
plot(trans.y ~ x2, main = "log(Price) vs Rainfall", xlab = "Rainfall (mm)", ylab = "log(Price)")
plot(trans.y ~ x3, main = "log(Price) vs Previous Rain",
     xlab = "Previous Rainfall (mm)", ylab = "log(Price)")
plot(trans.y ~ x4, main = "log(Price) vs Temperature",
     xlab = "Temperature (degrees C)", ylab = "log(Price)")

## Residual Plots e_i vs x_i
par(mfrow = c(2,2))
plot(resid(trans.fit)~x1, main = "Transformed Model Residuals vs Age",
     xlab = "Age (years)", ylab = "Residuals")
  abline(h = 0)
plot(resid(trans.fit)~x2, main = "Transformed Model Residuals vs Rainfall",
     xlab = "Rainfall (mm)", ylab = "Residuals")
	abline(h = 0)
plot(resid(trans.fit)~x3, main = "Transformed Model Residuals vs Previous Rain",
     xlab = "Previous Rainfall (mm)", ylab = "Residuals")
	abline(h = 0)
plot(resid(trans.fit)~x4, main = "Transformed Model vs Temperature",
     xlab = "Temperature (degrees C)", ylab = "Residuals")
	abline(h = 0)

```

We see that the residual plots for age and rainfall appear to be without a pattern. Previous rainfall and temperature (especially temperature) may not be linear.

We'll also take another look at the summary and ANOVA tables to get some quantitative information:
```{r}
summary(trans.fit)
summary(aov(trans.fit))
```
Note the increase of the R-Squared value to 0.8282 from 0.7356, which is a significant improvement. It's worth noting that based on the p-values in the coefficients table, we can see that all four explanatory variables are significant.


We'll run the Lag 1 Autocorrelation again to see if the independence of errors is reasonable for the new model:
```{r}
## 2. Check for Independence
	
##Lag 1 Autocorrelation
n = length(trans.y)
acf(resid(trans.fit), lag.max = 1, plot = FALSE)
2 / sqrt(n)

```

Since 0.417 > 0.384, independence of variables is still not necessarily reasonable. 

It's time to check the normality of residuals assumption for the new model with a q-q plot:
```{r}
## 3. Check for normality of Residuals

# q-q plot
z <- (trans.fit$residuals - mean(trans.fit$residuals) ) / sd(trans.fit$residuals)
qqnorm(z, sub = "Residuals (transformed)")
abline(a = 0, b = 1)


```

The q-q plot isn't quite linear, so the results aren't definitive. We can run a Shapiro-Wilk test for a more formal test to confirm:

```{r}
	## Shapiro-Wilk
  shapiro.test(trans.fit$residuals)

```

With a p-value of 0.3441, we cannot reject the null hypothesis that the residuals are linear. Still, we'd be wise to doubt this is an optimal model as the we see a clear pattern to the q-q plot points.


Finally, let's check for the equal variance of residuals with a plots of the residuals against fitted values:
```{r}

# residual plots
layout(matrix(c(1,2,3,4),2,2))
plot(trans.fit)

```

The residual plot appears to have no real pattern. It's likely that the errors of this model have equal variances. 

Overall, we see the new model is not perfect. However, it's definitely an improvement.

## Part 6: Analysis

As previously mentioned, the summary table for the log(y) transformed model showed that all four variables are significant, with the highest p-value being .024:

```{r}
summary(trans.fit)
```

The estimates column of the summary shows that Age, Previous Rain, and Temperature are positively correlated with price, and rain is negatively correlated with price. However, these values are extremely small (each change in one unit of the variable has a minimal effect on the log(price) when all variables are taken into condsideration).

All four of the basic model conditions are left with somewhat ambiguous results. This, combined with the R-squared value of 0.8282, which still leaves over 17% of the variance in price unexplained by the model, ought to make us question the adequacy of a linear model in the first place. It's likely that some other model will more effectively model the effects of these variables on the price of Bordeax Vintage. However, if a linear model is required, the log(Price) transformed model is almost certainly the best.